{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pathlib import Path \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "from transformers import (\n",
    "    BertForTokenClassification, \n",
    "    BertTokenizerFast, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.cwd().parent\n",
    "data_dir = base_dir / 'data'\n",
    "\n",
    "dataset_name = 'resume_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha\\nApplication Development Associat...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina\\nHyderabad, Telangana - ...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha\\nApplication Development Associat...   \n",
       "1  Afreen Jamadar\\nActive member of IIIT Committe...   \n",
       "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...   \n",
       "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...   \n",
       "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...   \n",
       "\n",
       "                                          annotation  extras  \n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...     NaN  \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...     NaN  \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...     NaN  \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...     NaN  \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_json(data_dir / dataset_name, lines=True)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/singhsourabh/Resume-NER/blob/master/utils.py\n",
    "\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "def convert_goldparse(dataturks_JSON_FilePath):\n",
    "    \"\"\"\n",
    "    Converts labeled data from a Dataturks JSON file format into spaCy's NER training format.\n",
    "    \n",
    "    Args:\n",
    "        dataturks_JSON_FilePath (str): Path to the Dataturks JSON file containing the labeled data.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples representing spaCy-compatible training data in the format:\n",
    "              [\n",
    "                  (\n",
    "                      \"content\",\n",
    "                      {\n",
    "                          \"entities\": [(start, end, entity_label), ...]\n",
    "                      }\n",
    "                  ),\n",
    "                  ...\n",
    "              ]\n",
    "        or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        training_data = []\n",
    "        \n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)  \n",
    "            text = data['content'].replace(\"\\n\", \" \")  \n",
    "            entities = []  \n",
    "            data_annotations = data.get('annotation', None)\n",
    "            \n",
    "            if data_annotations:\n",
    "                for annotation in data_annotations:\n",
    "                    point = annotation['points'][0]  \n",
    "                    labels = annotation['label']\n",
    "\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']  # Start position of the entity\n",
    "                        point_end = point['end']      # End position of the entity\n",
    "                        point_text = point['text']    # Text covered by this entity\n",
    "\n",
    "                        # Calculate differences for leading/trailing whitespaces in the entity text\n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        \n",
    "                        # Adjust start and end indices to remove extra whitespaces\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start += lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end -= rstrip_diff\n",
    "                        \n",
    "                        entities.append((point_start, point_end + 1, label))\n",
    "            \n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"\n",
    "    Cleans spaCy-compatible training data by removing leading and trailing white spaces\n",
    "    from entity spans.\n",
    "    \n",
    "    Args:\n",
    "        data (list): The training data in spaCy format with each entry as a tuple:\n",
    "                     [\n",
    "                         (\n",
    "                             \"content\",\n",
    "                             {\n",
    "                                 \"entities\": [(start, end, label), ...]\n",
    "                             }\n",
    "                         ),\n",
    "                         ...\n",
    "                     ]\n",
    "    \n",
    "    Returns:\n",
    "        list: The cleaned training data with precise entity spans after removing whitespace.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern to match whitespace characters\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "   \n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']  \n",
    "        valid_entities = []  \n",
    "\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start  # Initialize valid start position\n",
    "            valid_end = end      # Initialize valid end position\n",
    "\n",
    "            # Adjust valid_start to skip leading whitespace characters\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
    "                valid_start += 1\n",
    "            \n",
    "            # Adjust valid_end to skip trailing whitespace characters\n",
    "            while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            \n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\", \"Skills\", \"College Name\", \"Email Address\",\n",
    "             \"Designation\", \"Companies worked at\", \"Graduation Year\", \"Years of Experience\", \"Location\"]\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i: t for i, t in enumerate(tags_vals)}\n",
    "\n",
    "def get_label(offset, labels):\n",
    "    if offset[0] == 0 and offset[1] == 0:\n",
    "        return 'O'\n",
    "    for label in labels:\n",
    "        if offset[1] >= label[0] and offset[0] <= label[1]:\n",
    "            return label[2]\n",
    "    return 'O'\n",
    "\n",
    "def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data[0], max_length=max_len, return_offsets_mapping=True)\n",
    "    curr_sent = {'orig_labels': [], 'labels': []}\n",
    "\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "\n",
    "    if not is_test:\n",
    "        labels = data[1]['entities']\n",
    "        labels.reverse()\n",
    "        for off in tok['offset_mapping']:\n",
    "            label = get_label(off, labels)\n",
    "            curr_sent['orig_labels'].append(label)\n",
    "            curr_sent['labels'].append(tag2idx[label])\n",
    "        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n",
    "\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent\n",
    "\n",
    "\n",
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n",
    "        self.resume = resume\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.resume)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = process_resume(\n",
    "            self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(data['labels'], dtype=torch.long),\n",
    "            'orig_label': data['orig_labels']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = trim_entity_spans(convert_goldparse(data_dir / dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Samples : 187\n",
      "Number of Validation Samples : 33\n"
     ]
    }
   ],
   "source": [
    "TEST_SPLIT=0.15\n",
    "text = [_[0] for _ in cleaned_data]\n",
    "entities = [_[1] for _ in cleaned_data]\n",
    "\n",
    "train_text, valid_text, train_entities, valid_entities = train_test_split(text, \n",
    "                                                            entities, \n",
    "                                                            test_size=TEST_SPLIT, \n",
    "                                                            random_state=10\n",
    "                                                            )\n",
    "\n",
    "train_data = [(train_text[i], train_entities[i])   for i in range(len(train_text))]\n",
    "valid_data = [(valid_text[i], valid_entities[i])   for i in range(len(valid_text))]\n",
    "\n",
    "print(\"Number of Training Samples : {TRAIN_DATA}\".format(TRAIN_DATA=len(train_data)))\n",
    "print(\"Number of Validation Samples : {VALID_DATA}\".format(VALID_DATA=len(valid_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Ram Edupuganti Software Development Director - Oracle Inc  - Email me on Indeed: indeed.com/r/Ram-Edupuganti/3ecdecbcba549e21  • Offering over 22 years of rich national & international experience in developing & deploying software appplicatoins development, customized solutions across Oracle Fusion HCM Global Core HR, Global Payroll, Global Absences, Compensation and all Talent modules - Performance Management, Goals Management, Career Development, Succession Management & Talent Review Meeting • Directed planning, strategy development and implementation & business solution delivery with recent successful implantations at Schneider Electric, Macy's, Pike International, British Telcom. • Accomplished in relational data modeling, data warehouse design and implementation, object oriented design and development, developing transactional & business analytics in CRM, SCM and HCM domains from client server to cloud SaaS. • Led large-scale business application architecture and design efforts; developed and maintained solutions for various business functional areas; assisted in resolving integration and interface issues between various systems with focus on optimizing performance and scalability • Extensive experience in all aspects of project management including budgeting and cost optimization, risk assessments and control, technical feasibility studies, project scope definition, estimations & cost control & so on • Recruit, Develop and led high-performing teams with high motivation and enhanced cross- functional collaboration  WORK EXPERIENCE  Software Development Director  Oracle Inc -  April 2015 to Present  Oracle Inc -  January 1998 to Present  98 with Oracle Inc. Growth Path & Deputations:  Senior Development Manager  Oracle Inc -  December 2011 to March 2015  Senior Development Manager  Oracle Inc -  March 2010 to November 2011  https://www.indeed.com/r/Ram-Edupuganti/3ecdecbcba549e21?isid=rex-download&ikw=download-top&co=IN   Principal Applications Developer  Oracle Inc -  September 2004 to February 2010  Project Lead  Oracle Inc -  September 1999 to August 2004  Senior Software Developer  Oracle Inc -  January 1998 to August 1999  R-Systems -  Sacramento, CA -  January 1997 to January 1998  Key Result Areas: • Providing overall leadership to the entire project team, mapping clients' requirements, transforming requirements into stipulations and providing them best solutions within the scope of project requirements • Creating and managing the estimates, project plan, project schedule, resource allocation and expenses; monitoring and reporting on standards & performance targets • Ensuring the delivery of quality releases on schedules using continuous improvement initiatives; adhering to quality norms throughout the project implementation • Working on service strategy, transition, operations, process improvement, process management, team building, training, hiring and client relationship management • Identifying and implementing strategies for building team effectiveness by promoting a spirit of cooperation among the team members • Supporting continuous improvement by investigating alternatives and new technologies and presenting the same for architectural review • Liaising with stakeholders during the course of problem diagnoses, requirements gathering, detailed level design, development, system test and production implementation to ensure that optimal resolutions are achieved  KNOWLEDGE PURVIEW • Machine Learning Algorithms supervised, unsupervised, NLP, chat bot and deep learning with Python • Building transactional applications & analytical solutions in SaaS model • Oracle fusion role based data security set up and customization for complex security requirements • Reporting Layer (Subject Areas) for fusion HCM cloud for all HCM modules • Data Warehousing design methodologies, star and snowflake schema designs, aggregations • OBIEE, Data Visualization Desktop, BICS, DVCS, Oracle Analytics Cloud (OAC), Tableau • Design and build dashboards, KPIs using Oracle BI platform and OTBI, Standard Reports using BI Publisher • Design and build mappings, knowledge modules, load plans using Oracle Data Integrator (ODI)    • Relational data modeling, Object Oriented Modeling & Design, UML, SOAP • Oracle 10g, 11g, 12c • SQL, PL/SQL, HTML, XML, JAVA, JDK, J2EE, Oracle ADF, Oracle JDeveloper • Applications Development for Oracle CRM, Oracle ebusiness SCM, • Applications/database/SQL/Batch program performance tuning • UNIX, Windows, Linux  R-Systems -  April 1996 to January 1998  Growth Path & Deputations:  Department of Corrections -  Sacramento, CA -  February 1996 to January 1997  Secretary of State  Salem, Tamil Nadu -  December 1995 to February 1996  Oracle Corporation -  Redwood Shores, CA -  January 1995 to December 1995  EDUCATION  M.S. in Computer Engineering  Texas A&M University -  Kingsville, TX  1994  B.S. in Electronics & Communications Engineering  Nagarjuna University  1992  SKILLS  SOFTWARE DEVELOPMENT (3 years), STRUCTURED SOFTWARE (3 years), BUSINESS INTELLIGENCE (1 year), ORACLE (1 year), ARCHITECTURE (Less than 1 year)  ADDITIONAL INFORMATION  CORE COMPETENCIES  Software Development & Consulting  Oracle Fusion Applications Architecture and Functionality    Oracle Business Intelligence development and implementations  Technology Planning  Delivery Management  Client Engagements (Stakeholders/Business)  Continuous Process Enhancement  Agile/Scrum Methodologies  Project Management\",\n",
       " {'entities': [[0, 14, 'Name'],\n",
       "   [15, 44, 'Designation'],\n",
       "   [15, 44, 'Designation'],\n",
       "   [47, 57, 'Companies worked at'],\n",
       "   [47, 57, 'Companies worked at'],\n",
       "   [81, 125, 'Email Address'],\n",
       "   [143, 151, 'Years of Experience'],\n",
       "   [1563, 1592, 'Designation'],\n",
       "   [1594, 1604, 'Companies worked at'],\n",
       "   [1631, 1641, 'Companies worked at'],\n",
       "   [1678, 1688, 'Companies worked at'],\n",
       "   [1718, 1744, 'Designation'],\n",
       "   [1746, 1756, 'Companies worked at'],\n",
       "   [1789, 1815, 'Designation'],\n",
       "   [1817, 1827, 'Companies worked at'],\n",
       "   [1960, 1992, 'Designation'],\n",
       "   [1994, 2004, 'Companies worked at'],\n",
       "   [2041, 2053, 'Designation'],\n",
       "   [2055, 2065, 'Companies worked at'],\n",
       "   [2127, 2137, 'Companies worked at'],\n",
       "   [4189, 4259, 'Skills'],\n",
       "   [4262, 4282, 'UNKNOWN'],\n",
       "   [4285, 4355, 'Skills'],\n",
       "   [4484, 4504, 'Skills'],\n",
       "   [4812, 4840, 'Degree'],\n",
       "   [4842, 4880, 'College Name'],\n",
       "   [4882, 4886, 'Graduation Year'],\n",
       "   [4888, 4936, 'Degree'],\n",
       "   [4938, 4958, 'College Name'],\n",
       "   [4960, 4964, 'Graduation Year']]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(base_model_name, truncation=True)\n",
    "model = BertForTokenClassification.from_pretrained(base_model_name, num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500  # Set based on your requirements\n",
    "\n",
    "train_dataset = ResumeDataset(train_data, tokenizer, tag2idx, max_len)\n",
    "val_dataset = ResumeDataset(valid_data, tokenizer, tag2idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_tokens(tokenizer, tag2idx):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    pad_tok = vocab[\"[PAD]\"]\n",
    "    sep_tok = vocab[\"[SEP]\"]\n",
    "    cls_tok = vocab[\"[CLS]\"]\n",
    "    o_lab = tag2idx[\"O\"]\n",
    "    return pad_tok, sep_tok, cls_tok, o_lab\n",
    "\n",
    "\n",
    "\n",
    "pad_tok, sep_tok, cls_tok, o_lab = get_special_tokens(tokenizer, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    # Apply argmax to get the predicted class indices\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Initialize lists to store filtered predictions and true labels\n",
    "    valid_predictions, valid_labels = [], []\n",
    "\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        # Mask the predictions to exclude [CLS], [SEP], [PAD]\n",
    "        preds_mask = (label != pad_tok) & (label != sep_tok) & (label != cls_tok)\n",
    "\n",
    "        # Apply the mask to the predictions only\n",
    "        valid_pred = np.array(pred)[preds_mask]\n",
    "        valid_label = np.array(label)[preds_mask]\n",
    "\n",
    "        # Convert to tag names using idx2tag mapping\n",
    "        valid_predictions.append([idx2tag[p] for p in valid_pred])\n",
    "        valid_labels.append([idx2tag[l] for l in valid_label])\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(valid_labels, valid_predictions)\n",
    "    recall = recall_score(valid_labels, valid_predictions)\n",
    "    f1 = f1_score(valid_labels, valid_predictions)\n",
    "    accuracy = accuracy_score(valid_labels, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'ner_exp1'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=base_dir / f\"checkpoints/{experiment_name}\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=10_000,\n",
    "    logging_dir= base_dir / f\"checkpoints/{experiment_name}/logs\",\n",
    "    logging_steps=200,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.364806</td>\n",
       "      <td>0.618047</td>\n",
       "      <td>0.549149</td>\n",
       "      <td>0.581564</td>\n",
       "      <td>0.890943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.304992</td>\n",
       "      <td>0.671306</td>\n",
       "      <td>0.555373</td>\n",
       "      <td>0.607861</td>\n",
       "      <td>0.903156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.293687</td>\n",
       "      <td>0.632105</td>\n",
       "      <td>0.659528</td>\n",
       "      <td>0.645525</td>\n",
       "      <td>0.900251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279549</td>\n",
       "      <td>0.713220</td>\n",
       "      <td>0.576754</td>\n",
       "      <td>0.637769</td>\n",
       "      <td>0.909163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285981</td>\n",
       "      <td>0.647279</td>\n",
       "      <td>0.666118</td>\n",
       "      <td>0.656563</td>\n",
       "      <td>0.906852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.266024</td>\n",
       "      <td>0.674265</td>\n",
       "      <td>0.616228</td>\n",
       "      <td>0.643942</td>\n",
       "      <td>0.908173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.254174</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.623833</td>\n",
       "      <td>0.657979</td>\n",
       "      <td>0.912266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248865</td>\n",
       "      <td>0.665811</td>\n",
       "      <td>0.711075</td>\n",
       "      <td>0.687699</td>\n",
       "      <td>0.912662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249045</td>\n",
       "      <td>0.681084</td>\n",
       "      <td>0.717738</td>\n",
       "      <td>0.698930</td>\n",
       "      <td>0.914906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>0.689767</td>\n",
       "      <td>0.698465</td>\n",
       "      <td>0.694089</td>\n",
       "      <td>0.914708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.22910205523173013, metrics={'train_runtime': 37.5013, 'train_samples_per_second': 49.865, 'train_steps_per_second': 0.8, 'total_flos': 477215929080000.0, 'train_loss': 0.22910205523173013, 'epoch': 10.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 10. Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/nfs/home/scg1143/ATSChecker/ner-extraction/checkpoints/final_model/tokenizer_config.json',\n",
       " '/nfs/home/scg1143/ATSChecker/ner-extraction/checkpoints/final_model/special_tokens_map.json',\n",
       " '/nfs/home/scg1143/ATSChecker/ner-extraction/checkpoints/final_model/vocab.txt',\n",
       " '/nfs/home/scg1143/ATSChecker/ner-extraction/checkpoints/final_model/added_tokens.json',\n",
       " '/nfs/home/scg1143/ATSChecker/ner-extraction/checkpoints/final_model/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(base_dir / f\"checkpoints/final_model\")\n",
    "tokenizer.save_pretrained(base_dir / f\"checkpoints/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "resticted_lables = [\"UNKNOWN\", \"O\", \"Email Address\"]\n",
    "\n",
    "def tokenize_resume(text, tokenizer, max_len):\n",
    "    tok = tokenizer.encode_plus(\n",
    "        text, max_length=max_len, return_offsets_mapping=True)\n",
    "\n",
    "    curr_sent = dict()\n",
    "\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "\n",
    "    final_data = {\n",
    "        'input_ids': torch.tensor(curr_sent['input_ids'], dtype=torch.long),\n",
    "        'token_type_ids': torch.tensor(curr_sent['token_type_ids'], dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(curr_sent['attention_mask'], dtype=torch.long),\n",
    "        'offset_mapping': tok['offset_mapping']\n",
    "    }\n",
    "\n",
    "    return final_data\n",
    "\n",
    "def predict(model, tokenizer, idx2tag, device, test_resume, max_len):\n",
    "    model.eval()\n",
    "    data = tokenize_resume(test_resume, tokenizer, max_len)\n",
    "    input_ids, input_mask = data['input_ids'].unsqueeze(\n",
    "        0), data['attention_mask'].unsqueeze(0)\n",
    "    labels = torch.tensor([1] * input_ids.size(0),\n",
    "                          dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=input_mask,\n",
    "        )\n",
    "        logits = outputs[0]\n",
    "\n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    label_ids = np.argmax(logits, axis=2)\n",
    "\n",
    "    entities = []\n",
    "    for label_id, offset in zip(label_ids[0], data['offset_mapping']):\n",
    "        curr_id = idx2tag[label_id]\n",
    "        curr_start = offset[0]\n",
    "        curr_end = offset[1]\n",
    "        if curr_id not in resticted_lables:\n",
    "            if len(entities) > 0 and entities[-1]['entity'] == curr_id and curr_start - entities[-1]['end'] in [0, 1]:\n",
    "                entities[-1]['end'] = curr_end\n",
    "            else:\n",
    "                entities.append(\n",
    "                    {'entity': curr_id, 'start': curr_start, 'end': curr_end})\n",
    "    for ent in entities:\n",
    "        ent['text'] = test_resume[ent['start']:ent['end']]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'Name',\n",
       "  'start': 0,\n",
       "  'end': 28,\n",
       "  'text': 'Sivaganesh Selvakumar DevOps'},\n",
       " {'entity': 'Designation', 'start': 29, 'end': 39, 'text': 'Consultant'},\n",
       " {'entity': 'Companies worked at',\n",
       "  'start': 45,\n",
       "  'end': 60,\n",
       "  'text': 'Infosys Limited'},\n",
       " {'entity': 'Location', 'start': 62, 'end': 70, 'text': 'Chennai,'},\n",
       " {'entity': 'Designation', 'start': 312, 'end': 326, 'text': 'Ops Consultant'},\n",
       " {'entity': 'Companies worked at',\n",
       "  'start': 328,\n",
       "  'end': 335,\n",
       "  'text': 'Infosys'},\n",
       " {'entity': 'Designation', 'start': 1906, 'end': 1913, 'text': 'Analyst'},\n",
       " {'entity': 'Companies worked at',\n",
       "  'start': 1915,\n",
       "  'end': 1930,\n",
       "  'text': 'Infosys Limited'},\n",
       " {'entity': 'Location', 'start': 1934, 'end': 1942, 'text': 'Chennai,'},\n",
       " {'entity': 'Degree',\n",
       "  'start': 1990,\n",
       "  'end': 2022,\n",
       "  'text': 'Bachelor in \"Instrumentation and'},\n",
       " {'entity': 'Degree', 'start': 2031, 'end': 2042, 'text': 'Engineering'}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_text = valid_data[10][0]\n",
    "predict(model, \n",
    "        tokenizer, \n",
    "        idx2tag, \n",
    "        'cuda', \n",
    "        resume_text, \n",
    "        500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VPS', 's Late B. ')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[20][0][426:429], valid_data[20][0][430:440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VPS’s Late B. S. Deore College of Engineering'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[20][0][426:471]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.ner_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc5dcff28870a78261c6058eed80c552ebd7953d5071591ed305fed95c81e49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
